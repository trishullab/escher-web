<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="description" content="ðŸŒ€: Self-Evolving Visual Concept Library using Vision-Language Critics">
	<meta name="keywords"
		content="Escher, Visual Programming, Computer Vision, Context bottleneck Models, Scientific Discovery, Neurosymbolic Learning, Program Synthesis, Computer Vision">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Escher: Self-Evolving Visual Concept Library using Vision-Language Critics</title>

	<script>
		window.dataLayer = window.dataLayer || [];

		function gtag() {
			dataLayer.push(arguments);
		}

		gtag('js', new Date());
		gtag('config', 'G-PYVRSFMDRL');
	</script>

	<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

	<link rel="stylesheet" href="./static/css/bulma.min.css">
	<link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
	<link rel="stylesheet" href="./static/css/bulma-slider.min.css">
	<link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link rel="stylesheet" href="./static/css/index.css">
	<link rel="stylesheet" href="./static/css/scrollytelling.css">
	<link rel="icon" href="https://fav.farm/ðŸŒ€" type="image/x-icon">

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<script defer src="./static/js/fontawesome.all.min.js"></script>
	<script src="./static/js/bulma-carousel.min.js"></script>
	<script src="./static/js/bulma-slider.min.js"></script>
	<script src="./static/js/index.js"></script>
</head>

<body>

	<section class="hero">
		<div class="hero-body">
			<div class="container is-max-desktop">
				<div class="columns is-centered">
					<div class="column has-text-centered">
						<h1 class="title is-1 publication-title"><span class="escher">Escher</span>: Self-Evolving
							Visual Concept Library using Vision-Language Critics </h1>
						<div class="is-size-5 publication-authors">
							<span class="author-block">
								<a href="https://atharvas.net">Atharva Sehgal</a><sup>1</sup>,</span>
							<span class="author-block">
								<a href="https://www.linkedin.com/in/py95/">Patrick Yuan</a><sup>2</sup>,</span>
							<span class="author-block">
								<a href="https://acbull.github.io/">Ziniu Hu</a><sup>3</sup>,</span>
							<span class="author-block">
								<a href="https://www.cms.caltech.edu/people/yyue/">Yisong Yue</a><sup>3</sup>,</span>
							<span class="author-block">
								<a href="https://jenjsun.com/">Jennifer Sun</a><sup>2</sup>,</span>

							<span class="author-block">
								<a href="https://www.cs.utexas.edu/~swarat">Swarat Chaudhuri</a><sup>1</sup>,
							</span>
						</div>
						<div class="is-size-5 publication-authors">
							<span class="author-block"><sup>1</sup>UT Austin,</span>
							<span class="author-block"><sup>2</sup>Cornell University</span>
							<span class="author-block"><sup>3</sup>Caltech</span>
						</div>

						<div class="column has-text-centered">
							<div class="publication-links">
								<span class="link-block">
									<a href="https://arxiv.org/abs/????.?????"
										class="external-link button is-normal is-rounded is-dark">
										<span class="icon">
											<i class="fas fa-file-pdf"></i>
										</span>
										<span>Arxiv</span>
									</a>
								</span>
								<!-- Code Link. -->
								<span class="link-block">
									<a href="https://github.com/trishullab/escher"
										class="external-link button is-normal is-rounded is-dark">
										<span class="icon">
											<i class="fab fa-github"></i>
										</span>
										<span>Code</span>
									</a>
								</span>
								<span class="link-block">
									<a href="static/escher-slides.pdf"
										class="external-link button is-normal is-rounded is-dark">
										<span class="icon">
											<i class="fas fa-external-link-alt"></i>
										</span>
										<span>Short Slide Deck</span>
									</a>
								</span>
							</div>

						</div>
					</div>
				</div>
			</div>
		</div>
	</section>

	<section class="hero teaser">
		<div class="container is-max-desktop">
			<div class="hero-body">
				<img src="./static/images/teaser.svg" style="max-width: 100%; height: auto;" loading="eager" <h2 class="subtitle has-text-centered">
				Prior work in concept-bottleneck visual recognition aims to leverage
				discriminative visual concepts to enable more accurate object classification.
				<span class="escher">Escher</span> is an approach for iteratively evolving a visual concept library using feedback from a VLM
				critic to discover descriptive visual concepts.
				</h2>

			</div>
		</div>
	</section>


	<section class="section">
		<div class="container is-max-desktop">
			<!-- Abstract. -->
			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<h2 class="title is-3">Abstract</h2>
					<div class="content has-text-justified">
						<p>
							We study the problem of building a visual concept library for visual recognition. Building effective visual concept
							libraries is challenging, as manual definition is labor-intensive, while relying solely on LLMs for concept generation
							can result in concepts that lack discriminative power or fail to account for the complex interactions between them.
						</p>
						<p>
							Our approach, <span class="escher">Escher</span>, takes a library learning perspective to iteratively discover and improve visual concepts.
							Escher uses a vision-language model (VLM) as a critic to iteratively refine the concept library, including accounting for
							interactions between concepts and how they affect downstream classifiers. By leveraging the in-context learning
							abilities of LLMs and the history of performance using various concepts, <span class="escher">Escher</span> dynamically improves its concept
							generation strategy based on the VLM critic's feedback. 
						</p>
						<p>
							Notably, <span class="escher">Escher</span> does not require any human annotations, and is
							thus an automated plug-and-play framework. We empirically demonstrate the ability of <span class="escher">Escher</span> to learn a concept library
							for zero-shot, few-shot, and fine-tuning visual classification tasks. This work represents, to our knowledge, the first
							application of concept library learning to real-world visual tasks.
						</p>
					</div>
				</div>
			</div>
			<!--/ Abstract. -->
		</div>
	</section>

	<!-- Removed the scientific discovery scrollytelling section @ static/scientific-discovery.html.txt -->
	<section class="section">
		<div class="container">
			<div class="columns is-vcentered">
				<div class="column is-max-mobile is-max-tablet is-max-desktop is-max-widescreen article">
					<h3 class="title is-size-6-mobile is-size-4-tablet">Discovering visual concepts</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left">
						<p>
							In many scientific fields, perceptual reasoning doesn't come naturally. Sometimes the features of an image aren't obvious to the human eye. Other times the questions we pose demand more than instinct, requiring deliberate analysis. As a result, scientists must learn how to identify subtle traits by building domain knowledge and seeking constant feedback from peers. Let's look at a concrete example from ecology.
						</p>
						<p>
							In this iNaturalist exchange [<a href="https://www.inaturalist.org/observations/1970016">Source</a>], an experienced ecologist uploads a geo-tagged photo of a lizard, initially misidentifying it as a Florida Scrub Lizard (<em>Sceloporus woodi</em>). Another user, a trained herpetologist, corrects the identification, suggesting that the lizard is actually a Northern Curly-tailed Lizard (<em>Leiocephalus carinatus</em>). They catch a distinguishing morphological detail: Sceloporus (the family of the Florida Scrub Lizard) do not have a strongly <a href="https://en.wikipedia.org/wiki/Keeled_scales">keeled</a> tail. The experienced ecologist agrees with the correction and explains that they were focusing on the lizard's color patterning, rather than the scales.
						</p>
						<p>
							We are interesed in the question: <strong>how can machine perception systems learn to identify such subtle visual concepts?</strong>
						</p>
					</div>
				</div>
				<!-- Image. -->
				<div class="column content">
					<img src="static/concept-learning-in-the-wild.svg" loading="eager">
				</div>
			</div>
		</div>
	</section>

	<section class="section">
		<div class="container">
			<div class="columns is-vcentered">
				<div class="column is-max-mobile is-max-tablet is-max-desktop is-max-widescreen article">
					<h3 class="title is-size-6-mobile is-size-4-tablet">Visual Programming</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left">
						<p>
							Visual programming attempts to decompose complex perceptual reasoning problems into
							a logical combination of simpler perceptual tasks that can be solved using off-the-shelf vision
							foundation models. <a href="https://github.com/allenai/visprog">Such</a> <a href="https://viper.cs.columbia.edu/">exciting</a> <a href="https://glab-caltech.github.io/vadar/">works</a> leverage the code generation capabilities of large language models (LLMs) along with a pre-specified API of visual foundation models to generate code that can be executed to answer perceptual reasoning questions.
						</p>
					</div>
				</div>
				<!-- Image. -->
				<div class="column content">
					<video src="static/vipergpt.webm" autoplay loop muted playsinline></video>
				</div>
			</div>
		</div>
	</section>

	<section class="section">
		<div class="container">
			<h2 class="title is-2">Visual Programming's decoupling bottleneck</h2>
			<!-- Method. -->
			<div class="columns is-centered" id="pysr">
				<div class="column is-max-mobile is-max-tablet is-max-desktop is-max-widescreen article">
					<h3 class="title is-size-6-mobile is-size-4-tablet">Constructing visual programs for scientific images</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							However, such approaches inherently suffer from <em>the decoupling of the program synthesizer and the underlying vision foundation models</em>. 
							The program synthesizer is trained to generate deterministic code, and the stochastic nature of the vision foundation model is hidden away by the API. This decoupling leads to a disconnect between how the program synthesizer assumes the vision foundation model will behave and how it actually behaves on 
							real-world images.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">Our Hypothesis</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							<strong>Our main goal is to understand how we can better- 
								if we can overcome this decoubling by using the vision foundation model as a critic to guide the program synthesizer.</strong>
						<p>
							Our hypothesis is that the islands emerge because PySR (or any symbolic algorithm) can only
							sample programs that are <strong>syntactically close</strong> to the best program in each
							population.
							But syntactic closeness does not necessarily imply semantic closeness. Can we instead sample
							programs that are <strong>semantically close</strong> to the best programs in each
							population?
						</p>
					</div>
				</div>
				<!-- Image. -->
				<!-- Make sure image fits in the container. -->
				<div class="column content">
					<img src="static/visual-programming-frames/1.svg" id="updateableFigure" loading="eager">
				</div>
			</div>
		</div>
	</section>

	<section class="section">
		<div class="container">
			<h2 class="title is-2"><span class="escher">Escher</span>: Self-Evolving Visual Concept Library using Vision-Language Critics</h2>
			<!-- Method. -->
			<div class="columns is-centered" id="escher-iterations-loop">
				<div class="column is-max-mobile is-max-tablet is-max-desktop is-max-widescreen article">
					<h3 class="title is-size-6-mobile is-size-4-tablet">Task: Fine-grained Image Classification</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							<span class="escher">Escher</span> focuses on the task of fine-grained image classification.
							In this task, we're given an image and a set of classes (in natural language). Our goal
							is to classify the image into one of the classes.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">Concept Library Desiderata: (2) Symbolic
						Guidance</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							<strong>Symbolic Guidance</strong>: A concept can also guide the search for new programs.
							This arises naturally
							from the abstraction property: if a concept is a good abstraction of a set of equations, we
							should be able to
							use it to sample new equations that are similar to the ones in the set.
						</p>
						<p>
							This is particularly useful in scientific discovery, where scientists often have a
							preconception of how certain
							variables will interact with each other, but may not know the exact form of the
							relationship<em>
								(The example with gravitational waves is a little misleading as the theoretical
								prediction prompted the
								experimental discovery).
							</em>
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR's Three phases</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							LaSR consists of three phases: <em>Hypothesis Evolution</em>, <em>Concept Abstraction</em>,
							and <em>Concept Evolution</em>. In
							<em>Hypothesis Evolution</em>, we search for programs that best fit the data and are guided
							by concepts discovered in the previous iteration.
							<em>Concept Abstraction</em> deals with updating the concept library with new concepts
							derived from the best
							performing programs. Finally, <em>Concept Evolution</em> involves updating the concept
							library with new concepts that are implications of the discovered concepts.
						</p>
						<p>
							This is a self-amplifying loop: better programs lead to better concepts, which can, in turn,
							guide the search for even better programs.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR: Phase 1 Hypothesis Evolution</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							The hypothesis evolution searches for programs that best fit the data and are guided by
							concepts discovered in the previous iteration. To do this, we will <em>extend</em> PySR's
							search process to incorporate concept guidance.
							The next few slides will detail how we do this.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">Phase 1: Base Algorithm</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							We build upon PySR's multi-population genetic programming framework to integrate concept
							guidance.
							Let's begin by understanding PySR's main loop first. Assuming \(k\) programs in a single
							population, PySR starts with
							a randomly initialized set of programs \( \left( \{Pi_1^1, \dots \Pi_1^k \} \right) \),
							from which the best program is selected \( \left( \pi_1^\star \right) \)
							by evaluating its fitness on a supervised dataset. This program undergoes either mutation (a
							part of the program is resampled),
							crossover (a part of the program is swapped with a part of the second best program),
							simplification (the program is simplified), or
							optimization (the constants in the program are optimized). The generated program replaces
							the oldest program in the population, and the process
							repeats for a fixed number of iterations. Each iteration takes less than a second, and many
							such populations can be run in parallel.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">Phase 1: LLM Operations </h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							Integrating concept guidance into PySR involves augmenting all symbolic operations that
							generate new programs with
							an LLM zero shot query that mimics the symbolic operation it is replacing
						</p>
						<p>
							Instead of fully replacing the symbolic operations, we propose a hybrid approach where we
							replace the symbolic operation with the LLM
							zero shot query with probability \(p\) (usually 1%). This allows us to incorporate the
							concept guidance into the search process without
							sacrificing the 'local search' nature of PySR.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">Phase 1: Concept Library Integration</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							Each LLM zero shot query is conditioned on concepts discovered in the previous iteration.
							These concepts are stored in a concept library
							and ensure that the LLM zero shot query is relevant to the current search space.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR: Phase 2 Concept Abstraction</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							Next, the best performing programs are abstracted into concepts. We rely on another LLM zero
							shot query
							to "summarize" the programs into a natural language representation. This concept is then
							stored in the concept library.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR: Phase 3 Concept Evolution</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							Finally, the concept library is updated with new concepts that are implications of the
							discovered concepts. This is done by
							another LLM zero shot query that is conditioned on the discovered concepts.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR: Intuition (1)</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							Let's take a step back and understand the intuition behind LaSR. We noted that
							PySR's ends up with "islands" in the search space because it can only sample programs
							that are syntactically close to the best programs in each population. As LaSR builds upon
							PySR,
							we will also end up with these "islands" in the search space at the end of Phase 1.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR: Intuition (2)</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							In Phase 2, LaSR abstracts the best performing programs into concepts. These concepts serve
							as "bridges" between the "islands" in the search space.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR: Intuition (3)</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							We can now sample new programs conditioned on the discovered concepts. This allows us to
							explore
							new parts of the search space that were previously inaccessible. Since we retain the "local
							search"
							capabilities of PySR, intuitively, this allows us to explore more "islands" in the search
							space.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR: Intuition (4)</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							Furthermore, in Phase 3, we can sample more concepts that are derived from the discovered
							concepts.
							these concepts, in turn, increase the exploration of the search space in areas that were
							previously
							inaccessible.
					</div>
				</div>
				<!-- Image. -->
				<div class="column content">
					<img src="static/escher-iterations-frames/1.svg" id="updateableFigure" loading="eager">
				</div>
			</div>
		</div>
	</section>

	<section class="section">
		<div class="container">
			<h2 class="title is-2">LaSR: Results</h2>
			<!-- Method. -->
			<div class="columns is-centered" id="lasr-results">
				<div class="column is-max-mobile is-max-tablet is-max-desktop is-max-widescreen article">
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR: Feynman Equations</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							LaSR doesn't require human provided concepts. Hence, we can test LaSR against other
							symbolic regression algorithms on the Feynman equations; a collection of equations that
							describe empirical relationships from the <a
								href="https://www.feynmanlectures.caltech.edu/">Feynman Lectures Series</a> .
							We found that LaSR outperforms other symbolic regression algorithms on this benchmark.
						</p>
						<p>
							LaSR's performance is ultimately bottlenecked by the quality of the language guidance. To
							evaluate this,
							we conducted a set of cascading experiments where we varied the quality of the language
							guidance by (1) increasing
							the probability of replacing symbolic operations with LLM zero shot queries and (2) changing
							the backend LLM model.
							We found that even a small model like <a href="https://github.com/meta-llama/llama3">Llama 3
								8B</a> can provide
							sufficient guidance for LaSR to outperform other symbolic regression algorithms.
						</p>
						<p>
							More details on this set of experiments (and a set of experiments on an unseen synthetic
							dataset) can be found in our paper.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR: Feynman Equations with Hints</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							We also conducted an enhancement study where we provided LaSR with hints for the equations
							in the Feynman equations dataset. We found that LaSR with hints accelerated the search
							process and found the correct equation faster than LaSR without hints, even discovering
							some new equations.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR: Feynman Equations Qualitative Study</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							We also conducted a qualitative study on the equations discovered by LaSR. Here, we should
							Equation #10 from the Feynman
							equations dataset: Coulomb's law. LaSR and PySR both discover a high performing program for
							this equation. Coulomb's law
							is interesting because it embodies multiple concepts: (1) The force and the distance of the
							charges are inversely proportional and follow a
							"power law" trend, (2) In the scalar form, the force is proportional to the product of the
							charges and since multiplication is commutative,
							the order of the charges does not matter.
						</p>
						<p>
							<strong>PySR's equation:</strong> PySR's equation is unwieldy and simplifies to the correct
							form after about 10 manual steps of simplification. Also,
							as this equation requires more constants, it is more prone to optimization errors.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR: Feynman Equations Qualitative Study</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							<strong>LaSR's equation:</strong> LaSR's equation is much simpler and reduces to ground
							truth after four manual steps of simplification. Surprisingly,
							we notice that smaller models tend to produce simpler equations.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR: Feynman Equations Qualitative Study</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							LaSR produces two artifacts: an equation of best fit and a concept library. Here, we
							showcase snippets of the concept library generated for
							Coulomb's law. As a consequence of LLM training, the concepts are rather verbose and small
							relevant concepts are often buried in the middle of the
							generations.
						</p>
						<p>
							<strong>Limitations:</strong> As a consequence of using LLM zero shot queries, LaSR cannot
							guarantee the factuality or the correctness of the concepts in the concept library.
							Furthermore, concepts deemed to be "important" may be a consequence of the LLM model's
							training data and may mislead scientists.
							Addressing these concerns is an exciting direction for future work in LLM guided program
							induction.
						</p>
					</div>

					<h3 class="title is-size-6-mobile is-size-4-tablet">LLM Scaling Laws: Present methodology</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							To investigate LaSR's utility in finding novel and practical empirical trends,
							we investigate whether LaSR can discover novel LLM scaling laws on the BigBench dataset.
						</p>
						<p>
							Traditionally, to identify an LLM scaling law, practitioners must first manually posit a
							"skeleton
							equation" with a fixed set of known variables and unknown free parameters, and then optimize
							the unknown parameters based on a dataset of model hyperparameters and resulting dataset
							fitness.
						</p>
					</div>


					<h3 class="title is-size-6-mobile is-size-4-tablet">LLM Scaling Laws: Methodology with LaSR</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							Instead of starting with a predefined equation, we use LaSR to discover the skeleton
							equation that best fits various subsets of the BigBench dataset.
						</p>
						<p>
							Removing the need to manually posit a skeleton equation simplifies the methodology for
							finding scaling laws in many ways. (1) It removes human preconceptions about the expected
							relationships between hyperparameters. (2) It increases the number of variables and the type
							of variables human practitioners can jointly reason about. (3) It enables positing equations
							of much higher complexity and variable interdependence than otherwise possible.
						</p>
					</div>
					<h3 class="title is-size-6-mobile is-size-4-tablet">LaSR's LLM Scaling Law</h3>
					<div class="content is-size-7-mobile is-size-6-tablet has-text-left step">
						<p>
							LaSR discovers the following scaling law on the subset of BigBench:
							$$\texttt{score} = \frac{A}{\left( \frac{\texttt{train_steps}}{B}\right)^\texttt{#shots}} +
							E$$
							More details are in the paper but, in essence, this scaling law suggests that increasing the
							number of shots exponentially increases the model's performance for instances with less
							training data, while having diminishing gains as the number of training steps of the model
							increase.
						</p>
						<p>
							As the output artifacts of LaSR are interpretable, we can even augment existing scaling laws
							with the discovered empirical! More details/limitations are in the paper!
						</p>
					</div>

				</div>
				<!-- Image. -->
				<div class="column content">
					<img src="static/results-frames/1.svg" id="updateableFigure" loading="eager">
				</div>
			</div>
		</div>
	</section>

	<section class="section">
		<div class="container is-max-desktop">
			<div class="columns is-centered">
				<div class="column is-full-width">
					<h2 class="title is-3">Related Links</h2>

					<div class="content has-text-left">
						<p>
							This project would not be possible without the excellent work of the community. These are
							some relevant papers to better understand the
							premise of our work:
						</p>
						<ul>
							<li><a href="https://www.nature.com/articles/s41586-023-06924-6">FunSearch: Making new
									discoveries in mathematical sciences using Large Language Models</a> </li>
							<li><a href="https://arxiv.org/abs/2305.01582">Interpretable Machine Learning for Science
									with PySR and SymbolicRegression.jl</a> </li>
							<li><a href="https://arxiv.org/abs/2310.19791">LILO: Learning Interpretable Libraries by
									Compressing and Documenting Code</a> </li>
							<li><a href="https://arxiv.org/abs/1911.12247 ">LLM-SR: Scientific Equation Discovery via
									Programming with Large Language Models</a> </li>
							<li><a href="https://arxiv.org/abs/2210.05050 ">Neurosymbolic Programming for Science</a>
							</li>
						</ul>

					</div>
				</div>
			</div>

		</div>
	</section>


	<section class="section" id="BibTeX">
		<div class="container is-max-desktop content">
			<h2 class="title">BibTeX</h2>
			<p>
				If you found this post interesting, please read <a href="https://arxiv.org/abs/????.?????">our
					paper</a> for mathematical details and
				experimental results. You can cite our paper as follows:
			</p>
			<pre><code>@misc{grayeli2024symbolicregressionlearnedconcept,
	title={Symbolic Regression with a Learned Concept Library}, 
	author={Atharva Sehgal and Patrick Yuan and Ziniu Hu and Yisong Yue and Jennifer Sun and Swarat Chaudhuri},
	year={2025},
	eprint={????.?????},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/????.?????}, 
}</code></pre>
		</div>
	</section>

	<footer class="footer">
		<div class="container">
			<div class="content has-text-centered">
				<a class="icon-link" href="https://arxiv.org/abs/????.?????">
					<i class="fas fa-file-pdf"></i>
				</a>
				<a class="icon-link"
					href="https://github.com/trishullab/escher"
					class="external-link" disabled>
					<i class="fab fa-github"></i>
				</a>
			</div>
			<div class="columns is-centered">
				<div class="column is-8">
					<div class="content">
						<p>
							This template is based on the <a href="https://nerfies.github.io/">Nerfiles</a> project
							page.
							The source code is available <a href="https://github.com/nerfies/nerfies.github.io">here</a>
							and is
							licensed under a <a rel="license"
								href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
								Commons Attribution-ShareAlike 4.0 International License</a>. I also make heavy use of
							the
							<a href="https://github.com/russellsamora/scrollama">Scrollama.js</a> package. Please
							remember
							to cite either the <a href="https://nerfies.github.io/">Nerfiles</a> website or
							<a href="https://github.com/trishullab/escher-web">this website</a> if you use this
							template!
						</p>
					</div>
				</div>
			</div>
		</div>
	</footer>

	<script src="https://unpkg.com/d3@5.9.1/dist/d3.min.js"></script>
	<script src="./static/scrollama.js"></script>
	<script src="./static/js/scrollytelling.js"></script>
	<script>
		// Init scrollable sections.
		mobileCorrections();
		// init("#scientific-discovery");
		init("#pysr");
		init("#escher-iterations-loop");
		init("#lasr-results");
	</script>
</body>

</html>